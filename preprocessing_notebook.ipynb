{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from nnsplit import NNSplit\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import torch\n",
    "import pickle\n",
    "import h5py\n",
    "from io import BytesIO\n",
    "splitter = NNSplit.load(\"en\", use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb+srv://cdminix:LTEG2pfoDiKfH29M@cluster0.pdjrf.mongodb.net/Reviews_Data?retryWrites=true&w=majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.Reviews_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson.objectid import ObjectId\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, hdf5_file=\"data.hdf5\", input_dim=768, hash_dim=6, seed=42, chunksize=1_000):\n",
    "        self.planes = []\n",
    "        self.input_dim = input_dim\n",
    "        np.random.seed(seed)\n",
    "        for i in range(hash_dim):\n",
    "            v = np.random.rand(input_dim)\n",
    "            v_hat = v / np.linalg.norm(v)\n",
    "            self.planes.append(v_hat)\n",
    "    \n",
    "        self.planes = np.matrix(self.planes)\n",
    "        self.data = h5py.File(hdf5_file, \"w\")\n",
    "        self.chunksize = chunksize\n",
    "        self.buckets = {}\n",
    "    \n",
    "    # Returns LSH of a vector\n",
    "    def hash(self, vector):\n",
    "        hash_vector = np.where((self.planes @ vector) < 0, 1, 0)[0]\n",
    "        hash_string = \"\".join([str(num) for num in hash_vector])\n",
    "        return hash_string\n",
    "    \n",
    "    # Add vector to bucket\n",
    "    def add(self, vector):\n",
    "        hashed = self.hash(vector)\n",
    "        \n",
    "        if hashed not in self.buckets:\n",
    "            self.buckets[hashed] = []\n",
    "        \n",
    "        self.buckets[hashed].append(vector)\n",
    "        if len(self.buckets[hashed]) >= self.chunksize:\n",
    "            if hashed not in self.data:\n",
    "                self.data.create_dataset(hashed, (self.chunksize,self.input_dim), compression='gzip', dtype='float16', chunks=True, maxshape=(None,self.input_dim))\n",
    "            else:\n",
    "                hf = self.data[hashed]\n",
    "                hf.resize((hf.shape[0] + self.chunksize), axis=0)\n",
    "            self.data[hashed][-self.chunksize:] = self.buckets[hashed]\n",
    "            self.buckets[hashed] = []\n",
    "            \n",
    "    def flush(self):\n",
    "        for hashed, vectors in self.buckets.items():\n",
    "            list_size = len(vectors)\n",
    "            if hashed not in self.data:\n",
    "                self.data.create_dataset(hashed, (list_size,self.input_dim), compression='gzip', dtype='float16', chunks=True, maxshape=(None,self.input_dim))\n",
    "            else:\n",
    "                hf = self.data[hashed]\n",
    "                hf.resize((hf.shape[0] + list_size), axis=0)\n",
    "            self.data[hashed][-list_size:] = self.buckets[hashed]\n",
    "            self.buckets[hashed] = []\n",
    "    \n",
    "    # Returns bucket vector is in\n",
    "    def get(self, vector):\n",
    "        hashed = self.hash(vector)\n",
    "        \n",
    "        if hashed in self.data:\n",
    "            return self.data[hashed]\n",
    "        \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdminix/anaconda3/envs/apaut/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce58b8c32f4b2f9d1ee9d28ce7d3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11687765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10_000\n",
    "i = 0\n",
    "\n",
    "insert_thread = None\n",
    "\n",
    "lsh_store = LSH(chunksize=batch_size)\n",
    "\n",
    "for review in tqdm(db['review_data'].find(), total=db['review_data'].count()):\n",
    "    if i >= 100_000:\n",
    "        lsh_store.flush()\n",
    "        break\n",
    "    if i % batch_size == 0:\n",
    "        if i > 0:\n",
    "            review_l = []\n",
    "            sentence_l = []\n",
    "            start_index_l = []\n",
    "            end_index_l = []\n",
    "            for j, val in enumerate(splitter.split(texts)):\n",
    "                for k, sentence in enumerate(val):\n",
    "                    sentence = str(sentence)\n",
    "                    strip_sentence = sentence.strip()\n",
    "                    if len(strip_sentence) > 0:\n",
    "                        review_l.append(ids[j])\n",
    "                        sentence_l.append(strip_sentence)\n",
    "                        if k >= 1:\n",
    "                            start_index_l.append(end_index_l[-1] + 1)\n",
    "                        else:\n",
    "                            start_index_l.append(0)\n",
    "                        end_index_l.append(start_index_l[-1] + len(sentence))\n",
    "            embeddings = model.encode(sentence_l, convert_to_tensor=True)\n",
    "            embeddings = embeddings*10\n",
    "            insert_list = []\n",
    "            for indv_review, sentence, start_index, end_index in zip(\n",
    "                review_l,\n",
    "                embeddings,\n",
    "                start_index_l,\n",
    "                end_index_l\n",
    "            ):\n",
    "                lsh_store.add(sentence.numpy().astype('float16'))\n",
    "        texts = []\n",
    "        ids = []\n",
    "    texts.append(zlib.decompress(review['review_text']).decode())\n",
    "    ids.append(review['_id'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantization Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open('test.pkl', \"rb\") as f:\n",
    "  data = pickle.load(f)\n",
    "data_small = data.sample(100_000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000  #chunk row size\n",
    "list_df = [data_small[i:i+n] for i in range(0,data_small.shape[0],n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3505a37efc8b4e9086841050266e5fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = []\n",
    "orig_reviews = data_small.index.values.tolist()\n",
    "for df in tqdm(list_df):\n",
    "    corpus_embeddings += model.encode(df['review_text'].values, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "def run_query(queries, quant=32):\n",
    "\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    top_k = min(5, len(corpus_embeddings))\n",
    "    for query_text in queries:\n",
    "        query = model.encode(query_text, convert_to_tensor=True)\n",
    "        corpus = torch.stack(corpus_embeddings)\n",
    "\n",
    "        if quant == 16:\n",
    "            corpus = torch.tensor(corpus.numpy().astype('float16').astype('float32'))\n",
    "            query = torch.tensor(query.numpy().astype('float16').astype('float32'))\n",
    "        if quant == 8:\n",
    "            corpus = np.asarray(corpus * 128, dtype=np.int8).astype('float32')\n",
    "            query = np.asarray(query * 128, dtype=np.int8).astype('float32')\n",
    "\n",
    "        cos_scores = util.pytorch_cos_sim(query, corpus)[0]\n",
    "        cos_scores = cos_scores.cpu()\n",
    "\n",
    "        #We use torch.topk to find the highest 5 scores\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        print(\"\\n\\n======================\\n\\n\")\n",
    "        print(\"Query:\", query_text)\n",
    "        print(\"Quantization:\", quant)\n",
    "        print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            print(data.loc[orig_reviews[idx]]['review_text'], \"(Score: %.4f)\" % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: blew my socks off\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "I laughed my ass off. (Score: 0.5259)\n",
      "Blew me away!!!! (Score: 0.5193)\n",
      "Blew me away! Twisted and unique. (Score: 0.4619)\n",
      "Scared the hell out of me... (Score: 0.4187)\n",
      "Sucked. (Score: 0.4004)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: decent film\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Decent story (Score: 0.5028)\n",
      "Great book,really good movie (Score: 0.5020)\n",
      "spectacular novel (Score: 0.4965)\n",
      "very similar to the movie (Score: 0.4911)\n",
      "The movie is better. (Score: 0.4860)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quite bad\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Very bad. (Score: 0.8503)\n",
      "Not terrible, but pretty bad. (Score: 0.7357)\n",
      "Not great.... (Score: 0.7232)\n",
      "Not bad. Not great, but not bad. (Score: 0.6885)\n",
      "Not wonderful. (Score: 0.6870)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: could have more dogs\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Did not have enough fire-cats. Please add more fire-cats. (Score: 0.4206)\n",
      "Rather than write a review, I'll point you here. \n",
      " http://women.timesonline.co.uk/tol/li... \n",
      " A Factoid: Research shows that Dog owners are 9X more likely to survive one year after a heart attack than NonDog owners. (Cat owners are about even). I think there might be some adverse selection here as those who aren't mobile most likely have to find new homes for their dogs. \n",
      " It's worth reading but a little dry being so evidence based. (Score: 0.4074)\n",
      "i need to read more diverse books. (Score: 0.3755)\n",
      "If you even like dogs, you will love this book. (Score: 0.3704)\n",
      "could have been longer (Score: 0.3642)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: peperami\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Master Murakami (Score: 0.4711)\n",
      "Pesimo. (Score: 0.4697)\n",
      "ji (Score: 0.4218)\n",
      "Lame-o (Score: 0.3838)\n",
      "Polu gluko \n",
      " (Apo ta diegemata tou New Yorker) (Score: 0.3833)\n"
     ]
    }
   ],
   "source": [
    "run_query([\n",
    "    'blew my socks off',\n",
    "    'decent film',\n",
    "    'quite bad',\n",
    "    'could have more dogs',\n",
    "    'peperami'\n",
    "], quant=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: blew my socks off\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "I laughed my ass off. (Score: 0.5260)\n",
      "Blew me away!!!! (Score: 0.5193)\n",
      "Blew me away! Twisted and unique. (Score: 0.4619)\n",
      "Scared the hell out of me... (Score: 0.4187)\n",
      "Sucked. (Score: 0.4003)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: decent film\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Decent story (Score: 0.5028)\n",
      "Great book,really good movie (Score: 0.5020)\n",
      "spectacular novel (Score: 0.4965)\n",
      "very similar to the movie (Score: 0.4911)\n",
      "The movie is better. (Score: 0.4860)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quite bad\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Very bad. (Score: 0.8503)\n",
      "Not terrible, but pretty bad. (Score: 0.7357)\n",
      "Not great.... (Score: 0.7232)\n",
      "Not bad. Not great, but not bad. (Score: 0.6885)\n",
      "Not wonderful. (Score: 0.6870)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: could have more dogs\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Did not have enough fire-cats. Please add more fire-cats. (Score: 0.4206)\n",
      "Rather than write a review, I'll point you here. \n",
      " http://women.timesonline.co.uk/tol/li... \n",
      " A Factoid: Research shows that Dog owners are 9X more likely to survive one year after a heart attack than NonDog owners. (Cat owners are about even). I think there might be some adverse selection here as those who aren't mobile most likely have to find new homes for their dogs. \n",
      " It's worth reading but a little dry being so evidence based. (Score: 0.4074)\n",
      "i need to read more diverse books. (Score: 0.3755)\n",
      "If you even like dogs, you will love this book. (Score: 0.3704)\n",
      "could have been longer (Score: 0.3642)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: peperami\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Master Murakami (Score: 0.4711)\n",
      "Pesimo. (Score: 0.4697)\n",
      "ji (Score: 0.4218)\n",
      "Lame-o (Score: 0.3838)\n",
      "Polu gluko \n",
      " (Apo ta diegemata tou New Yorker) (Score: 0.3833)\n"
     ]
    }
   ],
   "source": [
    "run_query([\n",
    "    'blew my socks off',\n",
    "    'decent film',\n",
    "    'quite bad',\n",
    "    'could have more dogs',\n",
    "    'peperami'\n",
    "], quant=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: blew my socks off\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Blew me away!!!! (Score: 0.4751)\n",
      "I laughed my ass off. (Score: 0.4402)\n",
      "Blew me away! Twisted and unique. (Score: 0.4246)\n",
      "Scared the hell out of me... (Score: 0.3950)\n",
      "Creepy and abrupt! House of Leaves scared my pants off! Like literally my pants jumped off and ran away and I am now in my under ware!!!! (Score: 0.3657)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: decent film\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Great book,really good movie (Score: 0.5018)\n",
      "spectacular novel (Score: 0.4851)\n",
      "very similar to the movie (Score: 0.4850)\n",
      "Good book, so was the movie (Score: 0.4723)\n",
      "Decent story (Score: 0.4707)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quite bad\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Very bad. (Score: 0.8193)\n",
      "Not terrible, but pretty bad. (Score: 0.7040)\n",
      "Not great.... (Score: 0.6986)\n",
      "Not wonderful. (Score: 0.6644)\n",
      "Not bad. Not great, but not bad. (Score: 0.6587)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: could have more dogs\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Rather than write a review, I'll point you here. \n",
      " http://women.timesonline.co.uk/tol/li... \n",
      " A Factoid: Research shows that Dog owners are 9X more likely to survive one year after a heart attack than NonDog owners. (Cat owners are about even). I think there might be some adverse selection here as those who aren't mobile most likely have to find new homes for their dogs. \n",
      " It's worth reading but a little dry being so evidence based. (Score: 0.4084)\n",
      "Did not have enough fire-cats. Please add more fire-cats. (Score: 0.4032)\n",
      "i need to read more diverse books. (Score: 0.3754)\n",
      "I thought it was a really cute story. Would have loved to have read more. (Score: 0.3632)\n",
      "Lovely stories about recsued dogs. (Score: 0.3542)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: peperami\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Master Murakami (Score: 0.4521)\n",
      "Pesimo. (Score: 0.3630)\n",
      "What Sarah pi said. (Score: 0.3231)\n",
      "Hehehehehe...:P (Score: 0.3100)\n",
      "Polu gluko \n",
      " (Apo ta diegemata tou New Yorker) (Score: 0.3090)\n"
     ]
    }
   ],
   "source": [
    "run_query([\n",
    "    'blew my socks off',\n",
    "    'decent film',\n",
    "    'quite bad',\n",
    "    'could have more dogs',\n",
    "    'peperami'\n",
    "], quant=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9278)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05334cbfc29744a087a3d8d947c81f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = []\n",
    "orig_reviews = data_small.index.values.tolist()\n",
    "reviews = []\n",
    "sentences = []\n",
    "for df in tqdm(list_df):\n",
    "    sentence_l = []\n",
    "    for i, val in enumerate(splitter.split(df['review_text'].values)):\n",
    "        for sentence in val:\n",
    "            reviews.append(df.index.values[i])\n",
    "            sentence_l.append(str(sentence).strip())\n",
    "    sentences += sentence_l\n",
    "    corpus_embeddings += model.encode(sentence_l, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803918, 4803918, 4803918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(reviews), len(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbn                                                         NaN\n",
       "book_id                                                 18487371\n",
       "review_text    I did enjoy this book and I liked the characte...\n",
       "Name: 6753565, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[reviews[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = pd.DataFrame()\n",
    "vector_df['sentences'] = sentences\n",
    "vector_df['review_index'] = reviews\n",
    "vector_df['vector'] = corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.to_pickle('subset_vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "#corpus_embeddings = model.encode(sentences, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27600000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*2_300_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
