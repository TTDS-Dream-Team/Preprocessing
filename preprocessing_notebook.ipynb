{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from nnsplit import NNSplit\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import torch\n",
    "import pickle\n",
    "import h5py\n",
    "from io import BytesIO\n",
    "splitter = NNSplit.load(\"en\", use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = {\"_id\":{\"$oid\":\"601322d57f9d64d59ac2781f\"},\"book_id\":\"22875447\",\"isbn\":\"1455554790\",\"review_text\":{\"$binary\":{\"base64\":\"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\",\"subType\":\"00\"}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'I think this is a must read for any leader in a modern business. Google has done a lot of things right both in their products and also in how they run their company and build their culture, and this is a fairly detailed account of how they\\'ve built an impressive culture, and is written by someone who knows - their head of HR. I\\'m a little surprised he told as much as he did - but I suppose it will only help for recruiting. \\n Goodreads is now a subsidiary of Amazon, and I have spent significant time learning to integrate the best of Amazons culture with ours. And I\\'m happy to say that many - perhaps most - of the best practices listed in the book are also used by Amazon. Things like hiring people smarter than you, hiring committees and having objective people on them, committees to approve promotions, focusing on the two tails, and more. These don\\'t seem to be things all companies do yet - but should. \\n So while much of the practices were things I\\'m already doing or aware of - there was a lot I learned from the book too. Here are some of the bigger takeaways I had. \\n One of the more interesting ones was the notion to separate performance reviews from compensation discussions. This makes a lot of sense, is something we have already been making progress towards, and is something I\\'m going to think about more. \\n \"Traditional performance management systems make a big mistake. They combine two things that should be completely separate: performance evaluation and people development. Evaluation is necessary to distribute finite resources, like salary increases or bonus dollars. Development is just as necessary so people grow and improve.\" If you want people to grow, don\\'t have those two conversations at the same time. Make development a constant back-and-forth between you and your team members, rather than a year-end surprise.\" \\n Another one was giving managers a bi-annual scorecards from their directs on how they did on ~10 dimensions that Google has determined are the determinants of a great manager. And no surprise (but very important to keep in mind), the book found that \"manager quality was the single best predictor of whether employees would stay or leave, supporting the adage that people don\\'t quit companies, they quit bad managers.\" While we do a lot of surveys, we haven\\'t packaged up the managers feedback into a report like this, and I think that would be powerful. \\n Laszlo was impressive in citing lots of research to prove his points. It was one of my more favorite things about the book - he is clearly a student of human development. This led to lots of tidbits that apply pretty broadly, and which are great things to keep in mind when building a business. \\n The chapter on nudges was I think my favorite in the book. Pretty cool the depth to which they have taken these - reminds me a lot of the onboarding funnel analysis I\\'ve done for Goodreads - paying attention to where you can message timely, relevant, easily actionable messages that will result in people taking desired actions, and a/b testing the results. Pretty impressive they a/b test that kind of stuff at Google! Examples given were around lists on how to onboard someone as a manager, how to be onboarded as a newbie, how to get more people to save money earlier in life and enroll in the 401K program (his data here was impressive - on how people of the same income bracket vary widely on wealth accumulated in their lives based purely on how much they save when they are young), and how to get people to eat healthier by putting the healthier foods in the kitchens more prominently. \\n \"Nudges are an incredibly powerful mechanism for improving teams and organizations. They are also ideally suited to experimentation, so can be tested on smaller populations to fine-tune their results.\" \\n Laszlo did a great job of explaining a lot of the psychology behind nudges too. My favorite was the research about checklists, and story about how the Airforce found that even the smartest, best trained pilots can make mistakes, but having checklists reduces their error rates significantly. \\n \"I realized that management too is phenomenally complex. It\\'s a lot to ask of any leader to be a product visionary or a financial genius or a marketing wizard as well as an inspiring manager. But if we could reduce good management to a checklist, we wouldn\\'t need to invest millions of dollars in training, or try to convince people why one style of leadership is better than another. We wouldn\\'t have to change who they were. We could just change how they behave.\" \\n \"It turns out checklists really do work, even when the list is almost patronizingly simple. We\\'re human, and we sometimes forget the most basic things.\" \\n Another thing I loved was the focus on identifying the people who are best at a specific skill, and designing a program for them to teach that skill to others. G2G (Googlers 2 Googlers). \\n \"Giving employees the opportunity to teach gives them purpose. Even if they don\\'t find meaning in their regular jobs, passing on knowledge is both inspiring and inspirational.\" \\n I liked his descriptions of deliberate learning. He gave examples of asking after every meeting \"what did we learn and how could we do better in the future\". And the story about Tiger hitting golf balls at 4am in the rain was pretty sweet. \\n \"Ericsson refers to this as deliberate practice: intentional repetitions of similar, small tasks with immediate feedback, correction, and experimentation.\" \\n My favorite tidbit - which I know to be true but is something great to keep in mind - is how to motivate people: let them connect to the people their work is helping. \\n \"even a small connection to the people who benefit from your work not only improves productivity, it also makes people happier. And everyone wants their work to have purpose. \\n Bock, Laszlo (2015-04-07). Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead (pp. 340-341). Grand Central Publishing. Kindle Edition.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zlib.decompress(base64.b64decode(test_doc['review_text'][\"$binary\"][\"base64\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb+srv://cdminix:LTEG2pfoDiKfH29M@cluster0.pdjrf.mongodb.net/Reviews_Data?retryWrites=true&w=majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.Reviews_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson.objectid import ObjectId\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self,\n",
    "                 hdf5_file=\"data.hdf5\",\n",
    "                 input_dim=768,\n",
    "                 hash_dim=6,\n",
    "                 seed=42,\n",
    "                 chunksize=1_000,\n",
    "                 dtype='int8',\n",
    "        ):\n",
    "        self.planes = []\n",
    "        self.input_dim = input_dim\n",
    "        np.random.seed(seed)\n",
    "        for i in range(hash_dim):\n",
    "            v = np.random.rand(input_dim)\n",
    "            v_hat = v / np.linalg.norm(v)\n",
    "            self.planes.append(v_hat)\n",
    "    \n",
    "        self.planes = np.matrix(self.planes)\n",
    "        self.data = h5py.File(hdf5_file, \"w\")\n",
    "        self.chunksize = chunksize\n",
    "        self.buckets = {}\n",
    "        self.id_buckets = {}\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    # Returns LSH of a vector\n",
    "    def hash(self, vector):\n",
    "        hash_vector = np.where((self.planes @ vector) < 0, 1, 0)[0]\n",
    "        hash_string = \"\".join([str(num) for num in hash_vector])\n",
    "        return hash_string\n",
    "    \n",
    "    def quantize(self, vector_list):\n",
    "        vector_list = np.array(vector_list)\n",
    "        if self.dtype in ['float16', 'float32']:\n",
    "            return vector_list.astype(self.dtype)\n",
    "        if self.dtype == 'int8':\n",
    "            return np.asarray(vector_list * 128, dtype=np.int8)\n",
    "        raise ValueError(f'dtype needs to be float32, float16 or int8')\n",
    "    \n",
    "    # Add vector to bucket\n",
    "    def add(self, vector, i):\n",
    "        hashed = self.hash(vector)\n",
    "        \n",
    "        if hashed not in self.buckets:\n",
    "            self.buckets[hashed] = []\n",
    "            self.id_buckets[hashed] = []\n",
    "        \n",
    "        self.buckets[hashed].append(vector)\n",
    "        self.id_buckets[hashed].append(i)\n",
    "        \n",
    "        if len(self.buckets[hashed]) >= self.chunksize:\n",
    "            if hashed not in self.data:\n",
    "                self.data.create_dataset(hashed, (self.chunksize,self.input_dim), compression='gzip', dtype=self.dtype, chunks=True, maxshape=(None,self.input_dim))\n",
    "                self.data.create_dataset(hashed+'_id', (self.chunksize,), compression='gzip', dtype='int32', chunks=True, maxshape=(None,))\n",
    "            else:\n",
    "                hf = self.data[hashed]\n",
    "                hf_id = self.data[hashed+'_id']\n",
    "                hf.resize((hf.shape[0] + self.chunksize), axis=0)\n",
    "                hf_id.resize((hf_id.shape[0] + self.chunksize), axis=0)\n",
    "            self.data[hashed][-self.chunksize:] = self.quantize(self.buckets[hashed])\n",
    "            self.data[hashed+'_id'][-self.chunksize:] = self.id_buckets[hashed]\n",
    "            self.buckets[hashed] = []\n",
    "            self.id_buckets[hashed] = []\n",
    "            \n",
    "    def flush(self):\n",
    "        for hashed, vectors in self.buckets.items():\n",
    "            list_size = len(vectors)\n",
    "            if hashed not in self.data:\n",
    "                self.data.create_dataset(hashed, (list_size,self.input_dim), compression='gzip', dtype=self.dtype, chunks=True, maxshape=(None,self.input_dim))\n",
    "                self.data.create_dataset(hashed+'_id', (list_size,), compression='gzip', dtype='int32', chunks=True, maxshape=(None,))\n",
    "            else:\n",
    "                hf = self.data[hashed]\n",
    "                hf_id = self.data[hashed+'_id']\n",
    "                hf.resize((hf.shape[0] + list_size), axis=0)\n",
    "                hf_id.resize((hf_id.shape[0] + list_size), axis=0)\n",
    "            self.data[hashed][-list_size:] = self.quantize(self.buckets[hashed])\n",
    "            self.data[hashed+'_id'][-list_size:] = self.id_buckets[hashed]\n",
    "            self.buckets[hashed] = []\n",
    "            self.id_buckets[hashed] = []\n",
    "    \n",
    "    # Returns bucket vector is in\n",
    "    def get(self, vector):\n",
    "        hashed = self.hash(vector)\n",
    "        \n",
    "        if hashed in self.data:\n",
    "            return self.data[hashed]\n",
    "        \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db['sentence_data'].drop()\n",
    "    lsh_store.data.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1_000\n",
    "#i = 0\n",
    "#r_i = 0\n",
    "\n",
    "insert_thread = None\n",
    "\n",
    "\n",
    "#lsh_store = LSH(chunksize=batch_size)\n",
    "\n",
    "max_entries = db['review_data'].count()\n",
    "max_entries = 1_000_000\n",
    "\n",
    "for review in tqdm(db['review_data'].find(), total=max_entries-i):\n",
    "    if i >= max_entries:\n",
    "        lsh_store.flush()\n",
    "        break\n",
    "    if i % batch_size == 0:\n",
    "        if i > 0:\n",
    "            review_l = []\n",
    "            sentence_l = []\n",
    "            start_index_l = []\n",
    "            end_index_l = []\n",
    "            for j, val in enumerate(splitter.split(texts)):\n",
    "                for k, sentence in enumerate(val):\n",
    "                    sentence = str(sentence)\n",
    "                    strip_sentence = sentence.strip()\n",
    "                    if len(strip_sentence) > 0:\n",
    "                        review_l.append(ids[j])\n",
    "                        sentence_l.append(strip_sentence)\n",
    "                        if k >= 1:\n",
    "                            start_index_l.append(end_index_l[-1] + 1)\n",
    "                        else:\n",
    "                            start_index_l.append(0)\n",
    "                        end_index_l.append(start_index_l[-1] + len(sentence))\n",
    "            embeddings = model.encode(sentence_l, convert_to_tensor=True)\n",
    "            insert_list = []\n",
    "            for k, (indv_review, sentence, start_index, end_index) in enumerate(zip(\n",
    "                review_l,\n",
    "                embeddings,\n",
    "                start_index_l,\n",
    "                end_index_l\n",
    "            )):\n",
    "                lsh_store.add(sentence.numpy(), r_i)\n",
    "                insert_list.append({\n",
    "                    '_id': r_i,\n",
    "                    'review_id': indv_review,\n",
    "                    's': start_index,\n",
    "                    'e': end_index,\n",
    "                })\n",
    "                r_i += 1\n",
    "            insert_thread = Thread(target=db['sentence_data'].insert_many, args=(insert_list,))\n",
    "            insert_thread.start()\n",
    "        texts = []\n",
    "        ids = []\n",
    "    texts.append(zlib.decompress(review['review_text']).decode())\n",
    "    ids.append(review['_id'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_store.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantization Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open('test.pkl', \"rb\") as f:\n",
    "  data = pickle.load(f)\n",
    "data_small = data.sample(100_000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000  #chunk row size\n",
    "list_df = [data_small[i:i+n] for i in range(0,data_small.shape[0],n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3505a37efc8b4e9086841050266e5fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = []\n",
    "orig_reviews = data_small.index.values.tolist()\n",
    "for df in tqdm(list_df):\n",
    "    corpus_embeddings += model.encode(df['review_text'].values, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "def run_query(queries, quant=32):\n",
    "\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    top_k = min(5, len(corpus_embeddings))\n",
    "    for query_text in queries:\n",
    "        query = model.encode(query_text, convert_to_tensor=True)\n",
    "        corpus = torch.stack(corpus_embeddings)\n",
    "\n",
    "        if quant == 16:\n",
    "            corpus = torch.tensor(corpus.numpy().astype('float16').astype('float32'))\n",
    "            query = torch.tensor(query.numpy().astype('float16').astype('float32'))\n",
    "        if quant == 8:\n",
    "            corpus = np.asarray(corpus * 128, dtype=np.int8).astype('float32')\n",
    "            query = np.asarray(query * 128, dtype=np.int8).astype('float32')\n",
    "\n",
    "        cos_scores = util.pytorch_cos_sim(query, corpus)[0]\n",
    "        cos_scores = cos_scores.cpu()\n",
    "\n",
    "        #We use torch.topk to find the highest 5 scores\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        print(\"\\n\\n======================\\n\\n\")\n",
    "        print(\"Query:\", query_text)\n",
    "        print(\"Quantization:\", quant)\n",
    "        print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            print(data.loc[orig_reviews[idx]]['review_text'], \"(Score: %.4f)\" % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: blew my socks off\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "I laughed my ass off. (Score: 0.5259)\n",
      "Blew me away!!!! (Score: 0.5193)\n",
      "Blew me away! Twisted and unique. (Score: 0.4619)\n",
      "Scared the hell out of me... (Score: 0.4187)\n",
      "Sucked. (Score: 0.4004)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: decent film\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Decent story (Score: 0.5028)\n",
      "Great book,really good movie (Score: 0.5020)\n",
      "spectacular novel (Score: 0.4965)\n",
      "very similar to the movie (Score: 0.4911)\n",
      "The movie is better. (Score: 0.4860)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quite bad\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Very bad. (Score: 0.8503)\n",
      "Not terrible, but pretty bad. (Score: 0.7357)\n",
      "Not great.... (Score: 0.7232)\n",
      "Not bad. Not great, but not bad. (Score: 0.6885)\n",
      "Not wonderful. (Score: 0.6870)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: could have more dogs\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Did not have enough fire-cats. Please add more fire-cats. (Score: 0.4206)\n",
      "Rather than write a review, I'll point you here. \n",
      " http://women.timesonline.co.uk/tol/li... \n",
      " A Factoid: Research shows that Dog owners are 9X more likely to survive one year after a heart attack than NonDog owners. (Cat owners are about even). I think there might be some adverse selection here as those who aren't mobile most likely have to find new homes for their dogs. \n",
      " It's worth reading but a little dry being so evidence based. (Score: 0.4074)\n",
      "i need to read more diverse books. (Score: 0.3755)\n",
      "If you even like dogs, you will love this book. (Score: 0.3704)\n",
      "could have been longer (Score: 0.3642)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: peperami\n",
      "Quantization: 32\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Master Murakami (Score: 0.4711)\n",
      "Pesimo. (Score: 0.4697)\n",
      "ji (Score: 0.4218)\n",
      "Lame-o (Score: 0.3838)\n",
      "Polu gluko \n",
      " (Apo ta diegemata tou New Yorker) (Score: 0.3833)\n"
     ]
    }
   ],
   "source": [
    "run_query([\n",
    "    'blew my socks off',\n",
    "    'decent film',\n",
    "    'quite bad',\n",
    "    'could have more dogs',\n",
    "    'peperami'\n",
    "], quant=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: blew my socks off\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "I laughed my ass off. (Score: 0.5260)\n",
      "Blew me away!!!! (Score: 0.5193)\n",
      "Blew me away! Twisted and unique. (Score: 0.4619)\n",
      "Scared the hell out of me... (Score: 0.4187)\n",
      "Sucked. (Score: 0.4003)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: decent film\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Decent story (Score: 0.5028)\n",
      "Great book,really good movie (Score: 0.5020)\n",
      "spectacular novel (Score: 0.4965)\n",
      "very similar to the movie (Score: 0.4911)\n",
      "The movie is better. (Score: 0.4860)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quite bad\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Very bad. (Score: 0.8503)\n",
      "Not terrible, but pretty bad. (Score: 0.7357)\n",
      "Not great.... (Score: 0.7232)\n",
      "Not bad. Not great, but not bad. (Score: 0.6885)\n",
      "Not wonderful. (Score: 0.6870)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: could have more dogs\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Did not have enough fire-cats. Please add more fire-cats. (Score: 0.4206)\n",
      "Rather than write a review, I'll point you here. \n",
      " http://women.timesonline.co.uk/tol/li... \n",
      " A Factoid: Research shows that Dog owners are 9X more likely to survive one year after a heart attack than NonDog owners. (Cat owners are about even). I think there might be some adverse selection here as those who aren't mobile most likely have to find new homes for their dogs. \n",
      " It's worth reading but a little dry being so evidence based. (Score: 0.4074)\n",
      "i need to read more diverse books. (Score: 0.3755)\n",
      "If you even like dogs, you will love this book. (Score: 0.3704)\n",
      "could have been longer (Score: 0.3642)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: peperami\n",
      "Quantization: 16\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Master Murakami (Score: 0.4711)\n",
      "Pesimo. (Score: 0.4697)\n",
      "ji (Score: 0.4218)\n",
      "Lame-o (Score: 0.3838)\n",
      "Polu gluko \n",
      " (Apo ta diegemata tou New Yorker) (Score: 0.3833)\n"
     ]
    }
   ],
   "source": [
    "run_query([\n",
    "    'blew my socks off',\n",
    "    'decent film',\n",
    "    'quite bad',\n",
    "    'could have more dogs',\n",
    "    'peperami'\n",
    "], quant=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: blew my socks off\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Blew me away!!!! (Score: 0.4751)\n",
      "I laughed my ass off. (Score: 0.4402)\n",
      "Blew me away! Twisted and unique. (Score: 0.4246)\n",
      "Scared the hell out of me... (Score: 0.3950)\n",
      "Creepy and abrupt! House of Leaves scared my pants off! Like literally my pants jumped off and ran away and I am now in my under ware!!!! (Score: 0.3657)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: decent film\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Great book,really good movie (Score: 0.5018)\n",
      "spectacular novel (Score: 0.4851)\n",
      "very similar to the movie (Score: 0.4850)\n",
      "Good book, so was the movie (Score: 0.4723)\n",
      "Decent story (Score: 0.4707)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quite bad\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Very bad. (Score: 0.8193)\n",
      "Not terrible, but pretty bad. (Score: 0.7040)\n",
      "Not great.... (Score: 0.6986)\n",
      "Not wonderful. (Score: 0.6644)\n",
      "Not bad. Not great, but not bad. (Score: 0.6587)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: could have more dogs\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Rather than write a review, I'll point you here. \n",
      " http://women.timesonline.co.uk/tol/li... \n",
      " A Factoid: Research shows that Dog owners are 9X more likely to survive one year after a heart attack than NonDog owners. (Cat owners are about even). I think there might be some adverse selection here as those who aren't mobile most likely have to find new homes for their dogs. \n",
      " It's worth reading but a little dry being so evidence based. (Score: 0.4084)\n",
      "Did not have enough fire-cats. Please add more fire-cats. (Score: 0.4032)\n",
      "i need to read more diverse books. (Score: 0.3754)\n",
      "I thought it was a really cute story. Would have loved to have read more. (Score: 0.3632)\n",
      "Lovely stories about recsued dogs. (Score: 0.3542)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: peperami\n",
      "Quantization: 8\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Master Murakami (Score: 0.4521)\n",
      "Pesimo. (Score: 0.3630)\n",
      "What Sarah pi said. (Score: 0.3231)\n",
      "Hehehehehe...:P (Score: 0.3100)\n",
      "Polu gluko \n",
      " (Apo ta diegemata tou New Yorker) (Score: 0.3090)\n"
     ]
    }
   ],
   "source": [
    "run_query([\n",
    "    'blew my socks off',\n",
    "    'decent film',\n",
    "    'quite bad',\n",
    "    'could have more dogs',\n",
    "    'peperami'\n",
    "], quant=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
