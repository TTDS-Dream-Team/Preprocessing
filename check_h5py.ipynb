{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7322ea2efb5401da6a64ad71a120053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_file = h5py.File('data_M1.h5py', 'r')\n",
    "i_dict = {}\n",
    "for k in tqdm(test_file.keys()):\n",
    "    for chunk in test_file[k].iter_chunks():\n",
    "        #print(chunk)\n",
    "        #continue\n",
    "        if np.sum(test_file[k][chunk][-1]) == 0:\n",
    "            for i, v in enumerate(test_file[k][chunk]):\n",
    "                #print(v)\n",
    "                if np.sum(v) == 0:\n",
    "                    i_dict[k] = chunk[0].start+i\n",
    "                    break\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7584861"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_file = h5py.File('data_all.h5py', 'a')\n",
    "for k in tqdm(test_file.keys()):\n",
    "    if k not in all_file:\n",
    "        all_file.create_dataset(\n",
    "            k,\n",
    "            (1_000_000, 768),\n",
    "            compression=\"lzf\",\n",
    "            dtype='int8',\n",
    "            chunks=(10_000,768),\n",
    "            maxshape=(None, 768),\n",
    "        )\n",
    "        for chunk in test_file.iter_chunks():\n",
    "            if chunk[0] < i_dict[k]:\n",
    "                all_file[k][chunk] = test_file[k][chunk]\n",
    "            else:\n",
    "                for i, v in enumerate(test_file[k][chunk]):\n",
    "                    while np.sum(v) != 0:\n",
    "                        i_dict[k] = chunk[0].start+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\r\n",
      "\t\u001b[31mmodified:   .gitignore\u001b[m\r\n",
      "\t\u001b[31mmodified:   create_hdf5.py\u001b[m\r\n",
      "\t\u001b[31mmodified:   preprocessing_notebook.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31mUntitled.ipynb\u001b[m\r\n",
      "\t\u001b[31mcheck_cuda.py\u001b[m\r\n",
      "\t\u001b[31mcheck_h5py.ipynb\u001b[m\r\n",
      "\t\u001b[31mcreate_all.sh\u001b[m\r\n",
      "\t\u001b[31mcreate_vector_script.py\u001b[m\r\n",
      "\t\u001b[31mlast_index_100px.txt\u001b[m\r\n",
      "\t\u001b[31mpreprocess.ipynb\u001b[m\r\n",
      "\t\u001b[31mpreprocessing.py\u001b[m\r\n",
      "\t\u001b[31muntitled.txt\u001b[m\r\n",
      "\t\u001b[31mvisualize_lsh.ipynb\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 Bad Request"
     ]
    }
   ],
   "source": [
    "for chunk_size in (np.arange(20)+1)*10_000:\n",
    "    try:\n",
    "        file.close()\n",
    "    except:\n",
    "        pass\n",
    "    file = h5py.File('chunktest.h5py','w')\n",
    "    file.create_dataset(\n",
    "        'test',\n",
    "        (1_000_000, 768),\n",
    "        compression=\"lzf\",\n",
    "        dtype='int8',\n",
    "        chunks=(chunk_size,768),\n",
    "        maxshape=(None, 768),\n",
    "    )\n",
    "    np.random.seed(42)\n",
    "    file['test'][:] = np.random.randint(low=-127, high=127, size=(1_000_000, 768), dtype=np.int8)\n",
    "    buckets = []\n",
    "    vecs = []\n",
    "    times = []\n",
    "    with h5py.File('chunktest.h5py','r') as file:\n",
    "        for key in file.keys():\n",
    "            print(len(file[key]))\n",
    "\n",
    "            for c in file[key].iter_chunks():\n",
    "                start = time.time()\n",
    "                np.array(file[key][c])\n",
    "                end = time.time()\n",
    "                times.append(end-start)\n",
    "            break\n",
    "    print(f'chunksize: {chunk_size}')\n",
    "    print(np.sum(times), np.std(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
